# -*- coding: utf-8 -*-
"""Hands-on Activity 8.1 Aggregating Pandas DataFrames

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jkR3X_dtkjhmY8VejsUA1ALI715WbcuO

# CPE 311 -  Computational Thinking with Python

**Name:** Gwyneth D. Esperat

**Section:** CPE22S3

**Date:** March 27, 2024

**Github Link:** [Module 8](https://github.com/gwxnt07/CPE-311)

# 8.1.1 Intended Learning Outcomes
After this activity, the student should be able to:
* Demonstrate querying and merging of dataframes
* Perform advanced calculations on dataframes
* Aggregate dataframes with pandas and numpy
* Work with time series data

# 8.1.2 Resources
* Computing Environment using Python 3.x
* Attached Datasets (under Instructional Materials)

# 8.1.3 Procedures
The procedures can be found in the canvas module. Check the following under topics:

*   8.1 Weather Data Collection
* 8.2 Querying and Merging
* 8.3 Dataframe Operations
* 8.4 Aggregations
* 8.5 Time Series
"""

import pandas as pd

# Load the earthquakes data
earthquakes = pd.read_csv('/content/earthquakes.csv')

# Filter earthquakes in Japan with magType 'mb' and magnitude >= 4.9
earthquakes_japan = earthquakes[(earthquakes['place'].str.contains('Japan')) &
                                (earthquakes['magType'] == 'mb') &
                                (earthquakes['mag'] >= 4.9)]

earthquakes_japan

# Create bins for magnitudes
bins = pd.interval_range(start=0, end=int(earthquakes['mag'].max()) + 1, freq=1)
earthquakes['magnitude_bin'] = pd.cut(earthquakes[earthquakes['magType'] == 'ml']['mag'], bins=bins)

# Count earthquakes in each bin
magnitude_counts = earthquakes.groupby('magnitude_bin').size()

magnitude_counts

# Load the FAANG data
faang = pd.read_csv('/content/faang.csv', parse_dates=['date'])
faang.set_index('date', inplace=True)

# Group by ticker and resample to monthly frequency with aggregations
faang_monthly = faang.groupby('ticker').resample('M').agg({
    'open': 'mean',
    'high': 'max',
    'low': 'min',
    'close': 'mean',
    'volume': 'sum'
})

faang_monthly

# Create a crosstab
crosstab_max_magnitude = pd.crosstab(index=earthquakes['tsunami'], columns=earthquakes['magType'],
                                     values=earthquakes['mag'], aggfunc='max')

crosstab_max_magnitude

# Rolling 60-day aggregations by ticker
rolling_60d = faang.groupby('ticker').rolling(window='60D').agg({
    'open': 'mean',
    'high': 'max',
    'low': 'min',
    'close': 'mean',
    'volume': 'sum'
})

rolling_60d

# Pivot table for FAANG data
faang_pivot = faang.pivot_table(index='ticker', values=['open', 'high', 'low', 'close', 'volume'], aggfunc='mean')

faang_pivot

# Calculate Z-scores for Netflix data
netflix_data = faang[faang['ticker'] == 'NFLX'].select_dtypes(include=['float64', 'int64'])

z_scores = netflix_data.apply(lambda x: (x - x.mean()) / x.std())

z_scores

# Create an event dataframe
events_df = pd.DataFrame({
    'ticker': 'FB',
    'date': pd.to_datetime(['2018-07-25', '2018-03-19', '2018-03-20']),
    'event': ['Disappointing user growth announced after close.', 'Cambridge Analytica story', 'FTC investigation']
}).set_index(['date', 'ticker'])

# Merge with FAANG data
faang_reset = faang.reset_index()
events_merged = pd.merge(faang_reset, events_df, on=['date', 'ticker'], how='outer').set_index(['date', 'ticker'])

events_merged

"""# 8.1.4 Data Analysis
Provide some comments here about the results of the procedures.

### 1. Earthquakes in Japan
- **Observation**: A few earthquakes in Japan with a magnitude of 4.9 or greater and `magType` of `mb` were identified.
- **Comments**: These specific earthquakes might be significant enough to be studied for patterns or potential impacts on the region, considering their relatively higher magnitude.

### 2. Magnitude Bins with `magType` `ml`
- **Observation**: The majority of earthquakes fall into lower magnitude bins, with a sharp decrease in frequency as magnitude increases.
- **Comments**: This distribution is expected and aligns with the general understanding that lower magnitude earthquakes are far more common than higher magnitude ones.

### 3. Monthly Aggregations for FAANG Data
- **Observation**: Monthly aggregated data shows the variability in trading volumes and price ranges for each FAANG company over time.
- **Comments**: These trends can help investors understand historical performance, identify seasonal patterns, and make informed decisions.

### 4. Crosstab between `tsunami` and `magType`
- **Observation**: The crosstab revealed the maximum magnitudes associated with tsunamis for different `magTypes`.
- **Comments**: This analysis could be critical for disaster preparedness and understanding the correlation between earthquake characteristics and tsunami generation.

### 5. Rolling 60-day Aggregations for FAANG Data
- **Observation**: Rolling averages smooth out short-term fluctuations and highlight longer-term trends in stock prices and trading volumes.
- **Comments**: Investors might use this information to gauge the momentum of a stock and identify potential buying or selling opportunities.

### 6. Pivot Table for FAANG Data
- **Observation**: The pivot table provided a comparative overview of the average open, high, low, close prices, and volumes for FAANG stocks.
- **Comments**: This comparative analysis is useful for portfolio diversification, showing how different stocks behave over time relative to each other.

### 7. Z-scores for Netflix's Data
- **Observation**: Calculating Z-scores for Netflixâ€™s data highlights how each trading day's prices and volumes deviate from the mean.
- **Comments**: This can help identify outliers or unusual trading days that may be driven by specific events or announcements.

### 8. FAANG Data with Event Descriptions Added
- **Observation**: Incorporating significant events into the FAANG dataset provides context for stock price movements.
- **Comments**: Understanding the impact of specific events on stock performance is crucial for fundamental analysis and can guide investment strategies.

# 8.1.5 Supplementary Activity

Using the CSV files provided and what we have learned so far in this module complete the following exercises:

1. With the earthquakes.csv file, select all the earthquakes in Japan with a magType of mb and a magnitude of 4.9 or greater.
"""

import pandas as pd

earthquakes = pd.read_csv('/content/earthquakes.csv')
earthquakes

filtered_earthquakes = earthquakes[
    (earthquakes['place'].str.contains("Japan")) &
    (earthquakes['magType'] == 'mb') &
    (earthquakes['mag'] >= 4.9)
]

print(filtered_earthquakes)

"""2. Create bins for each full number of magnitude (for example, the first bin is 0-1, the second is 1-2, and so on) with a magType of ml and count how many are in each bin."""

bins = range(int(earthquakes['mag'].min()), int(earthquakes['mag'].max()) + 2)
mag_ml_df = earthquakes[earthquakes['magType'] == 'ml']

mag_ml_df = mag_ml_df.copy()
mag_ml_df['magnitude_bin'] = pd.cut(mag_ml_df['mag'], bins=bins, include_lowest=True, right=False)

magnitude_counts = mag_ml_df['magnitude_bin'].value_counts().sort_index()

print(magnitude_counts)

"""3. Using the faang.csv file, group by the ticker and resample to monthly frequency. Make the following aggregations:
* Mean of the opening price
* Maximum of the high price
* Minimum of the low price
* Mean of the closing price
* Sum of the volume traded

"""

import pandas as pd

# Corrected: Load the FAANG data
faang_df = pd.read_csv('/content/faang (1).csv', parse_dates=['date'])
faang_df.set_index('date', inplace=True)

# Group by ticker and resample to monthly frequency
faang_monthly = faang_df.groupby('ticker').resample('M').agg({
    'open': 'mean',
    'high': 'max',
    'low': 'min',
    'close': 'mean',
    'volume': 'sum'
})

print(faang_monthly.head())

"""4. Build a crosstab with the earthquake data between the tsunami column and the magType column. Rather than showing the frequency count, show the maximum
magnitude that was observed for each combination. Put the magType along the columns.
"""

earthquakes = pd.read_csv('/content/earthquakes.csv')

# Build a crosstab showing the maximum magnitude for each combination of tsunami and magType
tsunami_magType_max_mag = pd.crosstab(index=earthquakes['tsunami'], columns=earthquakes['magType'],
                                      values=earthquakes['mag'], aggfunc='max')

tsunami_magType_max_mag

"""5. Calculate the rolling 60-day aggregations of OHLC data by ticker for the FAANG data. Use the same aggregations as exercise no. 3.

"""

# Ensure the FAANG data is loaded and indexed by date
faang_df = pd.read_csv('/content/faang (1).csv', parse_dates=['date'])
faang_df.set_index('date', inplace=True)

# Calculate the rolling 60-day aggregations of OHLC data by ticker
faang_rolling_60 = faang_df.groupby('ticker').rolling(window='60D').agg({
    'open': 'mean',
    'high': 'max',
    'low': 'min',
    'close': 'mean',
    'volume': 'sum'
}).dropna()

faang_rolling_60.head()

"""6. Create a pivot table of the FAANG data that compares the stocks. Put the ticker in the rows and show the averages of the OHLC and volume traded data.

"""

# Create a pivot table for the FAANG data with ticker in the rows and averages of OHLC and volume
faang_pivot = faang_df.pivot_table(index='ticker', values=['open', 'high', 'low', 'close', 'volume'], aggfunc='mean')

faang_pivot

"""7. Calculate the Z-scores for each numeric column of Netflix's data (ticker is NFLX) using apply().

"""

# Filter the FAANG data for Netflix (NFLX) only
nflx_data = faang_df[faang_df['ticker'] == 'NFLX']

# Calculate Z-scores for each numeric column using apply()
nflx_z_scores = nflx_data.select_dtypes(include=['float64', 'int64']).apply(lambda x: (x - x.mean()) / x.std())

nflx_z_scores.head()

"""8. Add event descriptions:
* Create a dataframe with the following three columns: ticker, date, and event. The columns should have the following values:
* ticker: 'FB'
* date: ['2018-07-25', '2018-03-19', '2018-03-20']
event: ['Disappointing user growth announced after close.', 'Cambridge Analytica story', 'FTC investigation']
* Set the index to ['date', 'ticker']
* Merge this data with the FAANG data using an outer join

"""

# Create the events DataFrame
events_data = pd.DataFrame({
    'ticker': ['FB', 'FB', 'FB'],
    'date': ['2018-07-25', '2018-03-19', '2018-03-20'],
    'event': ['Disappointing user growth announced after close.', 'Cambridge Analytica story', 'FTC investigation']
})

# Convert 'date' to datetime format and set ['date', 'ticker'] as the index
events_data['date'] = pd.to_datetime(events_data['date'])
events_data.set_index(['date', 'ticker'], inplace=True)

# Ensure the FAANG data is in the correct format for merging
faang_df.reset_index(inplace=True)
faang_df.set_index(['date', 'ticker'], inplace=True)

# Merge the events data with the FAANG data using an outer join
faang_with_events = faang_df.join(events_data, on=['date', 'ticker'], how='outer').sort_index()

faang_with_events[faang_with_events['event'].notnull()]

"""9. Use the transform() method on the FAANG data to represent all the values in terms of the first date in the data. To do so, divide all the values for each ticker by the values for the first date in the data for that ticker. This is referred to as an index, and the data for the first date is the base (https://ec.europa.eu/eurostat/statistics-explained/index.php/ Beginners: Statisticalconcept-Indexandbaseyear). When data is in this format, we can easily see growth over time. Hint: transform() can take a function name."""

# Reset index of the FAANG data to work with transform()
faang_df.reset_index(inplace=True)
faang_df.set_index('date', inplace=True)

# Function to calculate the index relative to the first row in the group
def index_relative_to_first(row):
    return row / row.iloc[0]

# Group by ticker and apply the transformation to numeric columns
faang_indexed = faang_df.groupby('ticker').transform(index_relative_to_first)

# Include the 'ticker' column back into the faang_indexed DataFrame for clarity
faang_indexed['ticker'] = faang_df['ticker']

faang_indexed.head()

"""# Conclusion

In conclusion for the FAANG (Facebook, Apple, Amazon, Netflix, Google) and earthquake datasets yielded significant findings, showcasing the power of data manipulation and analysis. In examining earthquake data, we identified specific quakes in Japan based on magnitude and type, revealing the frequency distribution of these events and their potential to cause tsunamis. This analysis highlighted the commonality of lower magnitude earthquakes and provided insights into the relationship between earthquake characteristics and tsunamis. For the FAANG stocks, our investigation into monthly trading behaviors uncovered trends in prices and volumes, enhanced by a rolling 60-day aggregation for a deeper view of medium-term trends. A pivot table comparing the FAANG stocks illustrated variations in trading behavior, while the application of Z-scores to Netflix's data offered a statistical comparison of its performance. Additionally, merging specific event data with the FAANG dataset linked stock performance to major events, enriching the analysis. Transforming FAANG data to index values based on the first date facilitated the visualization of growth over time, underscoring the effectiveness of data analysis in drawing meaningful conclusions from complex datasets.
"""