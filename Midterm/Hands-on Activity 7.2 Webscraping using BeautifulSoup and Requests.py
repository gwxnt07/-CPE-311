# -*- coding: utf-8 -*-
"""Hands-on Activity 7.2 Webscraping using BeautifulSoup and Requests

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lvza_FLSIHlOTqQ7_851Z9RT3-fRAQno
"""

pip install opencv-python

!pip install opencv-python-headless

import cv2

key = cv2.waitKey(1)
webcam = cv2.VideoCapture(0)

while True:
    try:
        check, frame = webcam.read()
        print(check)  # prints true as long as the webcam is running
        print(frame)  # prints matrix values of each frame

        # Add a check to ensure frame is not None before displaying
        if frame is not None:
            cv2.imshow('frame', frame)

        key = cv2.waitKey(1)
        if key == ord('s'):
            cv2.imwrite(filename='saved_img.jpg', img=frame)
            webcam.release()
            img_new = cv2.imread('saved_img.jpg', cv2.IMREAD_GRAYSCALE)
            cv2.imshow('saved_img', img_new)
            cv2.waitKey(1650)
            cv2.destroyAllWindows()
            print("Processing image...")
            img_ = cv2.imread('saved_img.jpg', cv2.IMREAD_ANYCOLOR)
            print("Converting RGB image to grayscale...")
            gray = cv2.cvtColor(img_, cv2.COLOR_BGR2GRAY)
            print("Converted RGB image to grayscale...")
            print("Resizing image to 28x28 scale...")
            img_ = cv2.resize(gray, (28, 28))
            print("Resized...")
            img_resized = cv2.imwrite(filename='saved_img-final.jpg', img=img_)
            print("Image saved!")
            break
        elif key == ord('q'):
            print("Turning off camera.")
            webcam.release()
            print("Camera off.")
            print("Program ended.")
            cv2.destroyAllWindows()
            break
    except KeyboardInterrupt:
        print("Turning off camera.")
        webcam.release()
        print("Camera off.")
        print("Program ended.")
        cv2.destroyAllWindows()
        break

!pip3 install sounddevice

!pip3 install wavio

!pip3 install scipy

!apt-get install libportaudio2

import sounddevice as sd  # For recording audio
from scipy.io.wavfile import write  # For writing WAV files
import wavio as wv  # For writing WAV files

# Set the sampling frequency and recording duration
freq = 44100  # Sampling frequency in Hz
duration = 5  # Recording duration in seconds

try:
    # Start recording audio with the specified duration and sampling frequency
    recording = sd.rec(int(duration * freq), samplerate=freq, channels=2)

    # Wait for the recording to finish
    sd.wait()

    # Save the recorded audio as a WAV file using scipy.io.wavfile.write
    write("recording0.wav", freq, recording)

    # Save the recorded audio as a WAV file using wavio.write
    wv.write("recording1.wav", recording, freq, sampwidth=2)

    print("Recording saved successfully.")
except Exception as e:
    print("An error occurred:", e)

!pip install bs4

!pip install requests

import requests
from bs4 import BeautifulSoup

def getdata(url):
    r = requests.get(url)
    return r.text

htmldata = getdata("https://www.google.com/")
soup = BeautifulSoup(htmldata, 'html.parser')

for item in soup.find_all('img'):
    print(item['src'])

!pip install selenium

!pip install selenium
!apt-get update # to update ubuntu to correctly run apt install
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
import sys
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')

from selenium import webdriver
import time
import requests
import shutil
import os
import getpass
import urllib.request
import io
import time
from PIL import Image

def getImageUrls(name, totalImgs, driver):
    search_url = "https://www.google.com/search?q={q}&tbm=isch&tbs=sur%3Afc&hl=en&ved=0CAIQpwVqFwoTCKCa1c6s4-oCFQAAAAAdAAAAABAC&biw=1251&bih=568"
    driver.get(search_url.format(q=name))
    img_urls = set()
    img_count = 0
    results_start = 0
    while(img_count < totalImgs):
        # Extract actual images now
        scroll_to_end(driver)
        thumbnail_results = driver.find_elements_by_xpath("//img[contains(@class,'Q4LuWd')]")
        totalResults = len(thumbnail_results)
        print(f"Found: {totalResults} search results. Extracting links from {results_start}:{totalResults}")
        for img in thumbnail_results[results_start:totalResults]:
            img.click()
            time.sleep(2)
            actual_images = driver.find_elements_by_css_selector('img.n3VNCb')
            for actual_image in actual_images:
                if actual_image.get_attribute('src') and 'https' in actual_image.get_attribute('src'):
                    img_urls.add(actual_image.get_attribute('src'))
                    img_count = len(img_urls)
            if img_count >= totalImgs:
                print(f"Found: {img_count} image links")
                break
            else:
                print("Found:", img_count, "looking for more image links ...")
        load_more_button = driver.find_element_by_css_selector(".mye4qd")
        driver.execute_script("document.querySelector('.mye4qd').click();")
        results_start = len(thumbnail_results)
    return img_urls

def downloadImages(folder_path, file_name, url):
    try:
        image_content = requests.get(url).content
    except Exception as e:
        print(f"ERROR - COULD NOT DOWNLOAD {url} - {e}")
        return
    try:
        image_file = io.BytesIO(image_content)
        image = Image.open(image_file).convert('RGB')
        file_path = os.path.join(folder_path, file_name)
        with open(file_path, 'wb') as f:
            image.save(f, "JPEG", quality=85)
        print(f"SAVED - {url} - AT: {file_path}")
    except Exception as e:
        print(f"ERROR - COULD NOT SAVE {url} - {e}")

def saveInDestFolder(searchNames, destDir, totalImgs, driver):
    for name in list(searchNames):
        path = os.path.join(destDir, name)
        if not os.path.isdir(path):
            os.mkdir(path)
        print('Current Path', path)
        totalLinks = getImageUrls(name, totalImgs, driver)
        print('totalLinks', totalLinks)
        if totalLinks is None:
            print('images not found for :', name)
        else:
            for i, link in enumerate(totalLinks):
                file_name = f"{i:150}.jpg"
                downloadImages(path, file_name, link)

def scroll_to_end(driver):
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(2)  # Wait for page to load

# Example usage
from selenium import webdriver

# Initialize Selenium webdriver (make sure to have appropriate driver installed)
driver = webdriver.Chrome()

searchNames = ['cat']
destDir = '/content/drive/My Drive/Colab Notebooks/Dataset/'
totalImgs = 5

saveInDestFolder(searchNames, destDir, totalImgs, driver)

from requests import get
url = 'https://www.imdb.com/search/title?release_date=2017&sort=num_votes,desc&page=1'
response = get(url)
print(response.text[:500])

from bs4 import BeautifulSoup
html_soup = BeautifulSoup(response.text, 'html.parser')
headers = {'Accept-Language': 'en-US,en;q=0.8'}
type(html_soup)

movie_containers = html_soup.find_all('div', class_='lister-item mode-advanced')
print(type(movie_containers))  # Print the type of movie_containers
print(len(movie_containers))   # Print the number of movie containers found

# Check if movie_containers is not empty before accessing its first element
if movie_containers:
    first_movie = movie_containers[0]
    print(first_movie)
else:
    print("No movie containers found.")

first_movie = movie_containers[0]
first_movie

first_movie.div

first_movie.a

first_movie.h3

first_movie.h3.a

first_name = first_movie.h3.a.text
first_name

first_year = first_movie.h3.find('span', class_ = 'lister-item-year text-muted unbold')
first_year

first_year = first_year.text
first_year

first_movie.strong

first_imdb = float(first_movie.strong.text)
first_imdb

first_mscore = first_movie.find('span', class_ = 'metascore favorable')
first_mscore = int(first_mscore.text)
print(first_mscore)

first_mscore = first_movie.find('span', class_ = 'metascore favorable')
first_mscore = int(first_mscore.text)
print(first_mscore)

first_votes['data-value']

first_votes = int(first_votes['data-value'])

# Lists to store the scraped data in
names = []
years = []
imdb_ratings = []
metascores = []
votes = []

# Extract data from individual movie container
for container in movie_containers:
    # If the movie has Metascore, then extract:
    if container.find('div', class_='ratings-metascore') is not None:
        # The name
        name = container.h3.a.text
        names.append(name)

        # The year
        year = container.h3.find('span', class_='lister-item-year').text
        years.append(year)

        # The IMDB rating
        imdb = float(container.strong.text)
        imdb_ratings.append(imdb)

        # The Metascore
        m_score = container.find('span', class_='metascore').text
        metascores.append(int(m_score))

        # The number of votes
        vote = container.find('span', attrs={'name': 'nv'})['data-value']
        votes.append(int(vote))

import pandas as pd
test_df = pd.DataFrame({'movie': names,
'year': years,
'imdb': imdb_ratings,
'metascore': metascores,
'votes': votes
})
print(test_df.info())
test_df

import pandas as pd
from requests import get
from bs4 import BeautifulSoup
from time import time, sleep
from random import randint
from IPython.core.display import clear_output

pages = ['1', '2', '3', '4', '5']
years_url = ['2017', '2018', '2019', '2020']

# Lists to store data
names = []
years = []
imdb_ratings = []
metascores = []
votes = []

# Preparing the monitoring of the loop
start_time = time()
requests = 0

# For every year in the interval 2000-2017
for year_url in years_url:
    # For every page in the interval 1-5
    for page in pages:
        # Make a get request
        response = get('https://www.imdb.com/search/title?release_date=' + year_url +
                       '&sort=num_votes,desc&page=' + page)

        # Pause the loop
        sleep(randint(8, 15))

        # Monitor the requests
        requests += 1
        elapsed_time = time() - start_time
        print('Request:{}; Frequency: {} requests/s'.format(requests, requests / elapsed_time))
        clear_output(wait=True)

        # Parse the content of the request with BeautifulSoup
        page_html = BeautifulSoup(response.text, 'html.parser')

        # Select all the 50 movie containers from a single page
        mv_containers = page_html.find_all('div', class_='lister-item mode-advanced')

        # For every movie of these 50
        for container in mv_containers:
            # If the movie has a Metascore, then:
            if container.find('div', class_='ratings-metascore') is not None:
                # Scrape the name
                name = container.h3.a.text
                names.append(name)

                # Scrape the year
                year = container.h3.find('span', class_='lister-item-year').text
                years.append(year)

                # Scrape the IMDB rating
                imdb = float(container.strong.text)
                imdb_ratings.append(imdb)

                # Scrape the Metascore
                m_score = container.find('span', class_='metascore').text
                metascores.append(int(m_score))

                # Scrape the number of votes
                vote = container.find('span', attrs={'name': 'nv'})['data-value']
                votes.append(int(vote))

# Create a DataFrame with the scraped data
movie_ratings = pd.DataFrame({
    'movie': names,
    'year': years,
    'imdb': imdb_ratings,
    'metascore': metascores,
    'votes': votes
})

print(movie_ratings.info())
movie_ratings.head(10)

movie_ratings.tail(10)

movie_ratings.to_csv('/content/drive/My Drive/Colab Notebooks/Dataset/movie_ratings.csv')

movie_ratings['year'].unique()

movie_ratings.dtypes

movie_ratings['year'] = (movie_ratings.year.apply(lambda x:x.replace('(I)','')))

movie_ratings['year'].unique()

movie_ratings['year'] = (movie_ratings.year.apply(lambda x:x.replace('(II)','')))

movie_ratings['year'] = (movie_ratings.year.apply(lambda x:x.replace('(III)','')))

movie_ratings['year'].unique()

movie_ratings['year'] = (movie_ratings.year.apply(lambda x:x.replace('(','')))

movie_ratings['year'].unique()

movie_ratings['year'] = (movie_ratings.year.apply(lambda x:x.replace(')','')))

movie_ratings['year'].unique()

movie_ratings['year'] = movie_ratings['year'].astype(int)

movie_ratings['year'].unique()

movie_ratings.dtypes

movie_ratings.head(10)

movie_ratings.tail(10)

movie_ratings

"""# I'm sorry, sir, but I can't determine the real reason for my code errors. I'm still trying to figure out the problem.


"""