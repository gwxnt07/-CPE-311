{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data/dirty_data.csv')"
      ],
      "metadata": {
        "id": "TPAPAjxR_IPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "9kYgFpvl_NA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "yTwYz45r_OJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "sXBDVirF_PCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contain_nulls = df[\n",
        "df.SNOW.isnull() | df.SNWD.isna()\\\n",
        "| pd.isnull(df.TOBS) | pd.isna(df.WESF)\\\n",
        "| df.inclement_weather.isna()\n",
        "]\n",
        "contain_nulls.shape[0]"
      ],
      "metadata": {
        "id": "DY7btMgW_TNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contain_nulls.head(10)\n"
      ],
      "metadata": {
        "id": "-C5blSID_U8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.inclement_weather == 'NaN'].shape[0]\n"
      ],
      "metadata": {
        "id": "0-n2yC8I_U_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "df[df.inclement_weather == np.nan].shape[0]"
      ],
      "metadata": {
        "id": "2Zi2uNMt_eUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df[df.inclement_weather.isna()].shape[0]"
      ],
      "metadata": {
        "id": "i-B0y04d_gM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.SNWD.isin([-np.inf, np.inf])].shape[0]"
      ],
      "metadata": {
        "id": "fVYVGs9S_iEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_inf_count(df):\n",
        "\"\"\"Find the number of inf/-inf values per column in the dataframe\"\"\"\n",
        "return {\n",
        "col : df[df[col].isin([np.inf, -np.inf])].shape[0] for col in df.columns\n",
        "}\n",
        "\n",
        "get_inf_count(df)"
      ],
      "metadata": {
        "id": "OofonNMZ_lvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({\n",
        "'np.inf Snow Depth': df[df.SNWD == np.inf].SNOW.describe(),\n",
        "'-np.inf Snow Depth': df[df.SNWD == -np.inf].SNOW.describe()\n",
        "}).T"
      ],
      "metadata": {
        "id": "kRHctjg8_o42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='object')"
      ],
      "metadata": {
        "id": "TzqqyovR_vi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.duplicated()].shape[0]"
      ],
      "metadata": {
        "id": "_wzohcYV_wnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.duplicated(keep=False)].shape[0]"
      ],
      "metadata": {
        "id": "3lxrOQ0t_ymc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df[df.duplicated(['date', 'station'])].shape[0]"
      ],
      "metadata": {
        "id": "vYKYewJV_zPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.duplicated()].head()"
      ],
      "metadata": {
        "id": "2AVj3B-k_0gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save this information for later\n",
        "station_qm_wesf = df[df.station == '?'].WESF\n",
        "\n",
        "# sort ? to the bottom\n",
        "df.sort_values('station', ascending=False, inplace=True)\n",
        "\n",
        "# drop duplicates based on the date column keeping the first occurrence\n",
        "# which will be the valid station if it has data\n",
        "df_deduped = df.drop_duplicates('date').drop(\n",
        "# remove the station column because we are done with it\n",
        "# and WESF because we need to replace it later\n",
        "columns=['station', 'WESF']\n",
        "\n",
        ").sort_values('date').assign( # sort by the date\n",
        "# add back the WESF column which will be properly matched because of the index\n",
        "WESF=station_qm_wesf\n",
        ")\n",
        "\n",
        "df_deduped.shape"
      ],
      "metadata": {
        "id": "gjIS11KG_5Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.shape()"
      ],
      "metadata": {
        "id": "6mhWJbx0AJWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.dropna().shape"
      ],
      "metadata": {
        "id": "ZMVMKFWDAL6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.dropna(how='all').shape"
      ],
      "metadata": {
        "id": "OsQJATWqAQGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.dropna(\n",
        "how='all', subset=['inclement_weather', 'SNOW', 'SNWD']\n",
        ").shape"
      ],
      "metadata": {
        "id": "kcbtLkpbASHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.dropna(axis='columns', thresh=df_deduped.shape[0]*.75).columns"
      ],
      "metadata": {
        "id": "djtwAej-AV2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.loc[:'WESF'].fillna(0, inplace=True)\n",
        "df_deduped.head()"
      ],
      "metadata": {
        "id": "uq9OTFJ6AZt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.assign(\n",
        "TMAX=lambda x: x.TMAX.replace(5505, np.nan).fillna(method='ffill'),\n",
        "TMIN=lambda x: x.TMIN.replace(-40, np.nan).fillna(method='ffill')\n",
        ").head()"
      ],
      "metadata": {
        "id": "F9OkCr_uAgmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_deduped.assign(\n",
        "SNWD=lambda x: np.nan_to_num(x.SNWD)\n",
        ").head()"
      ],
      "metadata": {
        "id": "vMpULfD5_E3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daJABYQb5DIq"
      },
      "outputs": [],
      "source": [
        "df_deduped.assign(\n",
        "  TMAX=lambda x: x.TMAX.replace(5505, np.nan).fillna(x.TMAX.median()),\n",
        "  TMIN=lambda x: x.TMIN.replace(-40, np.nan).fillna(x.TMIN.median()),\n",
        "  # average of TMAX and TMIN\n",
        "  TOBS=lambda x: x.TOBS.fillna((x.TMAX + x.TMIN) / 2)\n",
        ").head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.assign(\n",
        "   # make TMAX and TMIN NaN where appropriate\n",
        "   TMAX=lambda x: x.TMAX.replace(5505, np.nan),\n",
        "   TMIN=lambda x: x.TMIN.replace(-40, np.nan)\n",
        ").set_index('date').apply(\n",
        "   # rolling calculations will be covered in chapter 4, this is a rolling 7 day median\n",
        "   # we set min_periods (# of periods required for calculation) to 0 so we always get a result\n",
        "   lambda x: x.fillna(x.rolling(7, min_periods=0).median())\n",
        ").head(10)\n"
      ],
      "metadata": {
        "id": "8akGZOwZ_E6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_deduped.assign(\n",
        "   # make TMAX and TMIN NaN where appropriate\n",
        "  TMAX=lambda x: x.TMAX.replace(5505, np.nan),\n",
        "  TMIN=lambda x: x.TMIN.replace(-40, np.nan),\n",
        "  date=lambda x: pd.to_datetime(x.date)\n",
        ").set_index('date').reindex(\n",
        "  pd.date_range('2018-01-01', '2018-12-31', freq='D')\n",
        ").apply(\n",
        "  lambda x: x.interpolate()\n",
        ").head(10)"
      ],
      "metadata": {
        "id": "3aRXihoQ_E8w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}