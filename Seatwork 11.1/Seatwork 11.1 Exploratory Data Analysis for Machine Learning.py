# -*- coding: utf-8 -*-
"""Seatwork 11.1 Exploratory Data Analysis for Machine Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pcsqf4EVxsbIJOhuo4lO2PAdchZm9Dww

#For Linear Regression Analysis:
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

wine_columns = [
    'Class', 'Alcohol', 'Malic Acid', 'Ash', 'Alcalinity of Ash', 'Magnesium',
    'Total Phenols', 'Flavanoids', 'Nonflavanoid Phenols', 'Proanthocyanins',
    'Color Intensity', 'Hue', 'OD280/OD315 of Diluted Wines', 'Proline'
]

data_file_path = '/content/wine.data'
wine_df = pd.read_csv(data_file_path, header=None, names=wine_columns)

print(wine_data.head())

print(wine_df.info())

print(wine_df.describe())

sns.pairplot(wine_df)
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Prepare the data
X = wine_df_scaled.drop(['Alcohol', 'Class'], axis=1)
y = wine_df_scaled['Alcohol']

# Splitting data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Linear Regression Model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict and evaluate the model
predictions = model.predict(X_test)
print('Mean Squared Error:', mean_squared_error(y_test, predictions))
print('R^2 Score:', r2_score(y_test, predictions))

plt.figure(figsize=(12, 10))
sns.heatmap(wine_df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.show()

missing_values = wine_data.isnull().sum()
print("Missing values in each column:\n", missing_values)

summary_statistics = wine_data.describe()
print("\nSummary Statistics:\n", summary_statistics)

sns.set(style="whitegrid")
fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(15, 20))
fig.subplots_adjust(hspace=0.5, wspace=0.5)
axes = axes.flatten()


for i, col in enumerate(wine_data.columns):
    sns.histplot(wine_data[col], kde=True, ax=axes[i], color='skyblue')
    axes[i].set_title(col, fontsize=14)
    axes[i].set_xlabel('')
    axes[i].set_ylabel('')

# Remove any empty plots
for ax in axes[len(wine_data.columns):]:
    fig.delaxes(ax)

plt.show()

# Check for missing values (if any)
print(wine_df.isnull().sum())

# Feature Scaling - Standardization (because features have different ranges)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
wine_df_scaled = pd.DataFrame(scaler.fit_transform(wine_df.drop(['Class'], axis=1)), columns=wine_df.columns[1:])
wine_df_scaled['Class'] = wine_df['Class']  # Add the non-scaled 'Class' column back

print(wine_df_scaled.head())

"""# For Logistic Regression Analysis

---


"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

column_names = [
    "symboling", "normalized_losses", "make", "fuel_type", "aspiration",
    "num_of_doors", "body_style", "drive_wheels", "engine_location",
    "wheel_base", "length", "width", "height", "curb_weight", "engine_type",
    "num_of_cylinders", "engine_size", "fuel_system", "bore", "stroke",
    "compression_ratio", "horsepower", "peak_rpm", "city_mpg", "highway_mpg", "price"
]
data = pd.read_csv('/content/imports-85.data', names=column_names)

print(data.head())

data.replace('?', np.nan, inplace=True)
data['normalized_losses'] = pd.to_numeric(data['normalized_losses'], errors='coerce')
data['price'] = pd.to_numeric(data['price'], errors='coerce')
numeric_columns = ['bore', 'stroke', 'horsepower', 'peak_rpm']
data[numeric_columns] = data[numeric_columns].apply(pd.to_numeric, errors='coerce')

data.dropna(subset=['price'], inplace=True)

for column in numeric_columns:
    data[column].fillna(data[column].mean(), inplace=True)

categorical_columns = ['make', 'fuel_type', 'aspiration', 'body_style', 'drive_wheels', 'engine_location']
data = pd.get_dummies(data, columns=categorical_columns)

data['normalized_losses'] = pd.to_numeric(data['normalized_losses'], errors='coerce')
data['bore'] = pd.to_numeric(data['bore'], errors='coerce')
data['stroke'] = pd.to_numeric(data['stroke'], errors='coerce')
data['horsepower'] = pd.to_numeric(data['horsepower'], errors='coerce')
data['peak_rpm'] = pd.to_numeric(data['peak_rpm'], errors='coerce')
data['price'] = pd.to_numeric(data['price'], errors='coerce')

numeric_data = data.select_dtypes(include=[np.number])

numeric_data.dropna(inplace=True)

plt.figure(figsize=(10, 8))
sns.heatmap(numeric_data.corr(), annot=True, fmt=".2f")
plt.show()

selected_columns = ['horsepower', 'city_mpg', 'highway_mpg', 'price', 'high_price']
sub_data = data[selected_columns]

sub_data['high_price'] = sub_data['high_price'].map({0: 'Low', 1: 'High'})

sns.pairplot(sub_data, hue='high_price', diag_kind='kde', markers=["o", "s"],
             plot_kws={'alpha': 0.6, 's': 80, 'edgecolor': 'k'},
             height=3)

plt.show()

data['high_price'] = data['high_price'].map({0: 'Low', 1: 'High'})

plt.figure(figsize=(8, 6))
sns.boxplot(x='high_price', y='horsepower', data=data)

plt.title('Horsepower Distribution by Price Category')
plt.xlabel('Price Category')
plt.ylabel('Horsepower')

plt.show()

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import pandas as pd

# Prepare data
X = data.drop(['price', 'high_price'], axis=1)
y = data['high_price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a pipeline with imputation, scaling, and logistic regression
pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler()),  # This will scale each feature to mean 0 and standard deviation 1
    ('logistic_regression', LogisticRegression(max_iter=10000))  # Increased max_iter
])

# Train the model
pipeline.fit(X_train, y_train)

# Predict and evaluate the model
predictions = pipeline.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(f'Accuracy of the logistic regression model: {accuracy:.2f}')